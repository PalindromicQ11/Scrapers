#title: Web_scrapying.py
#1: imports

Imports scrapy
from scrapy import LinkExtractors, Rules
from scrapy.crawler import CrawlerProcess 
Import {re, requests}

#2: Construct classes 
     # create objects, 
        # specify rules and customize settings

Class Scrape_websites(scrapy.crawl)--> str: 
     Name = []
     Allowed Domain = ['yahoo.com']
     urls = ['url'] ## This is where you specify the url you'd like to scrape
     User_agent = ['mozilla-firefox','chrome','IE']
      
     Rules = rules(linkedExtractor(allow=none, callback= parse_info))
     Custom_Settings = {'Cookies_enabled':'boolean',
                        'Cookies_debug':'boolean',
                        'Uri_format':['xml', 'html', 'json', 'csv', 'yaml', 'parquet']
                        'Allowed_format': ['xml','json','csv','yaml','parquet','html']}
    
     
     yield {
     ## 
     }
     
#3: Construct functions
    # Wrap inside a Try & and Error to help with debugging 

Def Scrape_links(self, response) --> str:
    While true:
    Try:
        links = response.css('ahref::attr('src')')
        drop_down = 
    Error as er: pass
        
Def scrape_page(self, response: list[str,int])--> str:
    While true:
    Try: 
        Article = article()
        article[0]= response.find_all(['th'][0])
        article[1]= response.find_all_item('
        
    Error as er: pass

#4 Concludes with crawlerprocess crawl classes & functions:
   if __name__ = "__main__"
   process = crawl_process(scrape_websites, scrape_page, scrape_links)
   process.start()
